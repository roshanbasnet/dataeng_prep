{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Caching and Persistance\n",
    "Spark Caching can be used to pull data into memory for faster processing. It is usually used when you are going to use the same data multiple times and for small datasets.\n",
    "\n",
    "There are two ways to persist an RDD in Spark:\n",
    "\n",
    "1. rdd.persist(): This method marks the RDD as persistable and caches the data in memory or disk depending on the storage level specified. By default, the storage level is MEMORY_ONLY. You can also specify other storage levels such as MEMORY_ONLY_SER, MEMORY_AND_DISK, etc.\n",
    "\n",
    "2. rdd.cache(): This method is equivalent to rdd.persist(StorageLevel.MEMORY_ONLY). It caches the data in memory.\n",
    "\n",
    "Note that if an RDD is not persisted or cached, Spark will recompute the RDD every time it is used in a subsequent transformation or action, which can be time-consuming. Persisting or caching the RDD allows Spark to reuse the data across multiple operations and can significantly improve performance.\n",
    "\n",
    "### Storange Types:\n",
    "1. MEMORY_ONLY: The default storage level. It stores the data in JVM heap memory, which is fast but is limited by the total amount of memory available to the Spark application.\n",
    "2. MEMORY_AND_DISK: It stores the data in JVM heap memory, but spills over to disk when there is not enough space. This is useful when the data does not fit in memory, but there is enough space on disk to store the data.\n",
    "3. MEMORY_ONLY_SER: It stores the data in serialized format in JVM heap memory. Serialization is a process of converting an object into a stream of bytes to store the object or transmit it to memory, a database, or a file. It is generally used when the data structure is complex as it reduces the size of the data.\n",
    "4. MEMORY_AND_DISK_SER: It stores the data in serialized format in JVM heap memory, but spills over to disk when there is not enough space.\n",
    "5. DISK_ONLY: It stores the data only on disk and is generally used when the data does not fit in memory, but there is enough space on disk to store the data.\n",
    "6. OFF_HEAP: It stores the data in Tachyon or an external storage system. Tachyon is a memory-centric distributed storage system that enables reliable data sharing across cluster frameworks.\n",
    "\n",
    "### RDD Unpersisting\n",
    "Spark automatically unpersists RDDs when they are garbage collected in least-recnelty used(LRU) fashion. However, you can also manually unpersist an RDD using the rdd.unpersist() method. This method unpersists the RDD from memory and disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_caching_and_persistance</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x287107575e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import spark context\n",
    "from pyspark.sql import SparkSession\n",
    "#create a spark context\n",
    "spark = SparkSession.builder.appName('pyspark_caching_and_persistance').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"file:///E:/DataEngineer_project/pyspark/test1.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Roshan', Age=27, Experience=3, Sallary=20000),\n",
       " Row(Name='Test', Age=26, Experience=1, Sallary=12000),\n",
       " Row(Name='sfsfk', Age=33, Experience=3, Sallary=43000),\n",
       " Row(Name='sdfsfsd', Age=4, Experience=4, Sallary=33333),\n",
       " Row(Name='Test', Age=None, Experience=None, Sallary=3333),\n",
       " Row(Name=None, Age=4, Experience=4, Sallary=None),\n",
       " Row(Name='Trettt', Age=None, Experience=None, Sallary=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.6423742999904789\n"
     ]
    }
   ],
   "source": [
    "# Getting total time taken to execute the query\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "data.count()\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Sallary: int]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caching the data\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  15.679735200013965\n"
     ]
    }
   ],
   "source": [
    "# Getting total time taken to execute the query\n",
    "import timeit\n",
    "\n",
    "sart = timeit.default_timer()\n",
    "data.count()\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Sallary: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  54.91073130001314\n"
     ]
    }
   ],
   "source": [
    "# Getting total time taken to execute the query\n",
    "import timeit\n",
    "\n",
    "sart = timeit.default_timer()\n",
    "data.count()\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Roshan', Age=27, Experience=3, Sallary=20000),\n",
       " Row(Name='Test', Age=26, Experience=1, Sallary=12000),\n",
       " Row(Name='sfsfk', Age=33, Experience=3, Sallary=43000),\n",
       " Row(Name='sdfsfsd', Age=4, Experience=4, Sallary=33333),\n",
       " Row(Name='Test', Age=None, Experience=None, Sallary=3333),\n",
       " Row(Name=None, Age=4, Experience=4, Sallary=None),\n",
       " Row(Name='Trettt', Age=None, Experience=None, Sallary=None)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Sallary: int]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using persist method\n",
    "import pyspark\n",
    "data.persist(pyspark.StorageLevel.DISK_ONLY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Sallary: int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Sallary: int]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using persist method\n",
    "import pyspark\n",
    "data.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f6a623343236782b3104312a38d731a14c125bf838f4c82db90ae7c3338775f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
