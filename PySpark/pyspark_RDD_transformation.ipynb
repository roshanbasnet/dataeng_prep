{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD Transformation\n",
    "* 1. map(), filter(), flatMap()\n",
    "* 2. Changes data from one form to another\n",
    "* 3. Lazy execution - Delayed execution of the transformation until an action is called  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Filter \n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of x_map: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Values of y_map: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
     ]
    }
   ],
   "source": [
    "## Map is used to apply a function to each element of the RDD\n",
    "x_map  = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y_map = x_map.map(lambda x: x * 2)\n",
    "\n",
    "print(f'Values of x_map: {x_map.collect()}')\n",
    "print(f'Values of y_map: {y_map.collect()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of x_flatMap: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Values of y_flatMap: [1, 2, 3, 201, 2, 4, 6, 202, 3, 6, 9, 203, 4, 8, 12, 204, 5, 10, 15, 205, 6, 12, 18, 206, 7, 14, 21, 207, 8, 16, 24, 208, 9, 18, 27, 209, 10, 20, 30, 210]\n"
     ]
    }
   ],
   "source": [
    "## FlatMap is used to apply a function to each element of the RDD and flatten the result. We can use  flatMap() to split each element of the RDD into multiple elements. \n",
    "x_flatMap  = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y_flatMap = x_flatMap.flatMap(lambda x: (x, x*2,x*3, 200+x))\n",
    "\n",
    "print(f'Values of x_flatMap: {x_flatMap.collect()}')\n",
    "print(f'Values of y_flatMap: {y_flatMap.collect()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of x: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Values of y: [6, 15, 34]\n"
     ]
    }
   ],
   "source": [
    "## MapPartitions is used to apply a function to each partition of the RDD.\n",
    "x = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)\n",
    "def f(iterator): yield sum(iterator)\n",
    "\n",
    "y= x.mapPartitions(f)\n",
    "\n",
    "print(f'Values of x: {x.collect()}')\n",
    "print(f'Values of y: {y.collect()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of x: [[1, 2, 3], [4, 5, 6], [7, 8, 9, 10]]\n",
      "Values of y: [[6], [15], [34]]\n"
     ]
    }
   ],
   "source": [
    "# glom() is used to convert the RDD into a list of lists i.e it will flaten elements of each partition into a list.\n",
    "print(f'Values of x: {x.glom().collect()}')\n",
    "print(f'Values of y: {y.glom().collect()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of x: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Values of y: [(0, 6), (1, 15), (2, 34)]\n"
     ]
    }
   ],
   "source": [
    "#mapPartitionsWithIndex is used to apply a function to each partition of the RDD. It also takes an integer value which represents the index of the partition.\n",
    "x = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)\n",
    "def f(splitIndex, iterator): yield (splitIndex, sum(iterator))\n",
    "\n",
    "y = x.mapPartitionsWithIndex(f)\n",
    "\n",
    "print(f'Values of x: {x.collect()}')\n",
    "print(f'Values of y: {y.collect()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[3, 4, 4, 5, 6, 10]\n"
     ]
    }
   ],
   "source": [
    "# sample() is used to return a new RDD that is a sample of the original RDD. It takes three parameters: withReplacement, fraction, and seed.\n",
    "# withReplacement: boolean value to indicate whether the sample is with replacement or not. take True for with replacement and False for without replacement.\n",
    "x = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y= x.sample(True, 0.5, 1)\n",
    "\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union() is used to combine two RDDs. It takes another RDD as a parameter and returns a new RDD.\n",
    "rdd_1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "rdd_2 = sc.parallelize([11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n",
    "\n",
    "rdd_1.union(rdd_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#internsection is used to return a new RDD that contains the common elements of the two RDDs.\n",
    "\n",
    "rdd_1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "rdd_2 = sc.parallelize([1, 2, 3, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n",
    "rdd_1.intersection(rdd_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 1, 9, 2, 10, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct() is used to return a new RDD that contains distinct elements of the original RDD.\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', <pyspark.resultiterable.ResultIterable at 0x1d193a13eb0>),\n",
       " ('C', <pyspark.resultiterable.ResultIterable at 0x1d193cdeb30>),\n",
       " ('A', <pyspark.resultiterable.ResultIterable at 0x1d193cdec80>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GroupBy is used to group the elements of the RDD based on the given function. It returns a tuple of the key and the corresponding values.\n",
    "x = sc.parallelize([('A',1),('B',2),('C',3),('A',4),('B',5),('C',6)])\n",
    "x.groupBy(lambda x: x[0]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f6a623343236782b3104312a38d731a14c125bf838f4c82db90ae7c3338775f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
