{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast Variables\n",
    "\n",
    "Broadcast Variables are a mechanism in Apache Spark to allow efficient sharing of read-only variables between different nodes in a cluster. The main idea behind Broadcast Variables is to cache a read-only variable on each node instead of shipping a copy of it with every task. This can help reduce the amount of network I/O and increase the performance of a Spark application.\n",
    "\n",
    "A Broadcast Variable is created using the SparkContext.broadcast method and can be used in Spark operations like map and reduce as a regular variable. The value of a Broadcast Variable is stored on each node and is accessible by tasks executing on that node.\n",
    "\n",
    "Broadcast variable can be accessed by broadcast.value. The broadcast variable is read-only and can only be used in Spark operations. It cannot be updated.\n",
    "\n",
    "broadcast_v.unpersist() is used to remove the broadcast variable from the worker nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1750d745d50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Broadcast Variable\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"James\", \"\", \"Smith\", \"36636\", \"M\", 3000),\n",
    "        (\"Michael\", \"Rose\", \"\", \"40288\", \"M\", 4000),\n",
    "        (\"Robert\", \"\", \"Williams\", \"42114\", \"M\", 4000),\n",
    "]\n",
    "\n",
    "rdd_data = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', '', 'Smith', '36636', 'M', 3000),\n",
       " ('Michael', 'Rose', '', '40288', 'M', 4000),\n",
       " ('Robert', '', 'Williams', '42114', 'M', 4000)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#broadcast variable\n",
    "brodcast_var = spark.sparkContext.broadcast([\"M\", \"F\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M', 'F']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accessing broadcast variable\n",
    "brodcast_var.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "brodcast_var.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Broadcast in module pyspark.broadcast object:\n",
      "\n",
      "class Broadcast(typing.Generic)\n",
      " |  Broadcast(sc: Optional[ForwardRef('SparkContext')] = None, value: Optional[~T] = None, pickle_registry: Optional[ForwardRef('BroadcastPickleRegistry')] = None, path: Optional[str] = None, sock_file: Optional[BinaryIO] = None)\n",
      " |  \n",
      " |  A broadcast variable created with :meth:`SparkContext.broadcast`.\n",
      " |  Access its value through :attr:`value`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from pyspark.context import SparkContext\n",
      " |  >>> sc = SparkContext('local', 'test')\n",
      " |  >>> b = sc.broadcast([1, 2, 3, 4, 5])\n",
      " |  >>> b.value\n",
      " |  [1, 2, 3, 4, 5]\n",
      " |  >>> sc.parallelize([0, 0]).flatMap(lambda x: b.value).collect()\n",
      " |  [1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n",
      " |  >>> b.unpersist()\n",
      " |  \n",
      " |  >>> large_broadcast = sc.broadcast(range(10000))\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Broadcast\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sc: Optional[ForwardRef('SparkContext')] = None, value: Optional[~T] = None, pickle_registry: Optional[ForwardRef('BroadcastPickleRegistry')] = None, path: Optional[str] = None, sock_file: Optional[BinaryIO] = None)\n",
      " |      Should not be called directly by users -- use :meth:`SparkContext.broadcast`\n",
      " |      instead.\n",
      " |  \n",
      " |  __reduce__(self) -> Tuple[Callable[[int], ForwardRef('Broadcast[T]')], Tuple[int]]\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  destroy(self, blocking: bool = False) -> None\n",
      " |      Destroy all data and metadata related to this broadcast variable.\n",
      " |      Use this with caution; once a broadcast variable has been destroyed,\n",
      " |      it cannot be used again.\n",
      " |      \n",
      " |      .. versionchanged:: 3.0.0\n",
      " |         Added optional argument `blocking` to specify whether to block until all\n",
      " |         blocks are deleted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      blocking : bool, optional\n",
      " |          Whether to block until unpersisting has completed\n",
      " |  \n",
      " |  dump(self, value: ~T, f: <class 'BinaryIO'>) -> None\n",
      " |  \n",
      " |  load(self, file: <class 'BinaryIO'>) -> ~T\n",
      " |  \n",
      " |  load_from_path(self, path: str) -> ~T\n",
      " |  \n",
      " |  unpersist(self, blocking: bool = False) -> None\n",
      " |      Delete cached copies of this broadcast on the executors. If the\n",
      " |      broadcast is used after this is called, it will need to be\n",
      " |      re-sent to each executor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      blocking : bool, optional\n",
      " |          Whether to block until unpersisting has completed\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  value\n",
      " |      Return the broadcasted value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[~T],)\n",
      " |  \n",
      " |  __parameters__ = (~T,)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(brodcast_var)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators\n",
    "\n",
    "Accumulators are a special type of variables in Spark that can be used to perform aggregations on the worker nodes and return the result to the driver program. They are used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.\n",
    "\n",
    "Accumulators are variables in Apache Spark that can be used to implement counters (simple or summarized data) or sums in a parallel and fault-tolerant manner. They are write-only variables from the viewpoint of the worker nodes and are initialized on the driver node. The worker nodes can only add to the value of the accumulator, but they cannot read its value. Only the driver node can read the final value of the accumulator after an action has been executed on the RDD.\n",
    "\n",
    "Accumulators are commonly used to implement global counters in Spark, where multiple nodes need to update the same variable. The main advantage of using accumulators is that they are resilient to failures, as they only keep track of updates on the successful nodes, while lost updates on failed nodes are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum_v = spark.sparkContext.accumulator(0)\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "rdd.foreach(lambda x: accum_v.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Accumulator in module pyspark.accumulators object:\n",
      "\n",
      "class Accumulator(typing.Generic)\n",
      " |  Accumulator(aid: int, value: ~T, accum_param: 'AccumulatorParam[T]')\n",
      " |  \n",
      " |  A shared variable that can be accumulated, i.e., has a commutative and associative \"add\"\n",
      " |  operation. Worker tasks on a Spark cluster can add values to an Accumulator with the `+=`\n",
      " |  operator, but only the driver program is allowed to access its value, using `value`.\n",
      " |  Updates from the workers get propagated automatically to the driver program.\n",
      " |  \n",
      " |  While :class:`SparkContext` supports accumulators for primitive data types like :class:`int` and\n",
      " |  :class:`float`, users can also define accumulators for custom types by providing a custom\n",
      " |  :py:class:`AccumulatorParam` object. Refer to its doctest for an example.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> a = sc.accumulator(1)\n",
      " |  >>> a.value\n",
      " |  1\n",
      " |  >>> a.value = 2\n",
      " |  >>> a.value\n",
      " |  2\n",
      " |  >>> a += 5\n",
      " |  >>> a.value\n",
      " |  7\n",
      " |  >>> sc.accumulator(1.0).value\n",
      " |  1.0\n",
      " |  >>> sc.accumulator(1j).value\n",
      " |  1j\n",
      " |  >>> rdd = sc.parallelize([1,2,3])\n",
      " |  >>> def f(x):\n",
      " |  ...     global a\n",
      " |  ...     a += x\n",
      " |  >>> rdd.foreach(f)\n",
      " |  >>> a.value\n",
      " |  13\n",
      " |  >>> b = sc.accumulator(0)\n",
      " |  >>> def g(x):\n",
      " |  ...     b.add(x)\n",
      " |  >>> rdd.foreach(g)\n",
      " |  >>> b.value\n",
      " |  6\n",
      " |  \n",
      " |  >>> rdd.map(lambda x: a.value).collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |  Traceback (most recent call last):\n",
      " |      ...\n",
      " |  Py4JJavaError: ...\n",
      " |  \n",
      " |  >>> def h(x):\n",
      " |  ...     global a\n",
      " |  ...     a.value = 7\n",
      " |  >>> rdd.foreach(h) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |  Traceback (most recent call last):\n",
      " |      ...\n",
      " |  Py4JJavaError: ...\n",
      " |  \n",
      " |  >>> sc.accumulator([1.0, 2.0, 3.0]) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |  Traceback (most recent call last):\n",
      " |      ...\n",
      " |  TypeError: ...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Accumulator\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __iadd__(self, term: ~T) -> 'Accumulator[T]'\n",
      " |      The += operator; adds a term to this accumulator's value\n",
      " |  \n",
      " |  __init__(self, aid: int, value: ~T, accum_param: 'AccumulatorParam[T]')\n",
      " |      Create a new Accumulator with a given initial value and AccumulatorParam object\n",
      " |  \n",
      " |  __reduce__(self) -> Tuple[Callable[[int, ~T, ForwardRef('AccumulatorParam[T]')], ForwardRef('Accumulator[T]')], Tuple[int, ~T, ForwardRef('AccumulatorParam[T]')]]\n",
      " |      Custom serialization; saves the zero value from our AccumulatorParam\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add(self, term: ~T) -> None\n",
      " |      Adds a term to this accumulator's value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  value\n",
      " |      Get the accumulator's value; only usable in driver program\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[~T],)\n",
      " |  \n",
      " |  __parameters__ = (~T,)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(accum_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum_v.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    accum_v.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.foreach(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD - Transformations - Joins\n",
    "* join()\n",
    "* leftOuterJoin()\n",
    "* rightOuterJoin()\n",
    "* fullOuterJoin()\n",
    "* cartesian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([('C',1),('B',2),('A',3),('A',4),('B',5),('C',6)])\n",
    "rdd2 = sc.parallelize([('A',7),('B',8),('C',9),('D',10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join act as inner join\n",
    "inner_join = rdd1.join(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (2, 8)),\n",
       " ('B', (5, 8)),\n",
       " ('C', (1, 9)),\n",
       " ('C', (6, 9)),\n",
       " ('A', (3, 7)),\n",
       " ('A', (4, 7))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner_join.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (2, 8)),\n",
       " ('B', (5, 8)),\n",
       " ('D', (None, 10)),\n",
       " ('C', (1, 9)),\n",
       " ('C', (6, 9)),\n",
       " ('A', (3, 7)),\n",
       " ('A', (4, 7))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#right outer join\n",
    "\n",
    "rdd1 = sc.parallelize([('C',1),('B',2),('A',3),('A',4),('B',5),('C',6)])\n",
    "rdd2 = sc.parallelize([('A',7),('B',8),('C',9),('D',10)])\n",
    "\n",
    "rdd1.rightOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', (2, 8)),\n",
       " ('B', (5, 8)),\n",
       " ('C', (1, 9)),\n",
       " ('C', (6, 9)),\n",
       " ('A', (3, 7)),\n",
       " ('A', (4, 7))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lerft outer join\n",
    "rdd1.leftOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('C', 1), ('A', 7)),\n",
       " (('C', 1), ('B', 8)),\n",
       " (('C', 1), ('C', 9)),\n",
       " (('C', 1), ('D', 10)),\n",
       " (('B', 2), ('A', 7)),\n",
       " (('B', 2), ('B', 8)),\n",
       " (('B', 2), ('C', 9)),\n",
       " (('B', 2), ('D', 10)),\n",
       " (('A', 3), ('A', 7)),\n",
       " (('A', 3), ('B', 8)),\n",
       " (('A', 3), ('C', 9)),\n",
       " (('A', 3), ('D', 10)),\n",
       " (('A', 4), ('A', 7)),\n",
       " (('A', 4), ('B', 8)),\n",
       " (('A', 4), ('C', 9)),\n",
       " (('A', 4), ('D', 10)),\n",
       " (('B', 5), ('A', 7)),\n",
       " (('B', 5), ('B', 8)),\n",
       " (('B', 5), ('C', 9)),\n",
       " (('B', 5), ('D', 10)),\n",
       " (('C', 6), ('A', 7)),\n",
       " (('C', 6), ('B', 8)),\n",
       " (('C', 6), ('C', 9)),\n",
       " (('C', 6), ('D', 10))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cartesian product(cross join)\n",
    "rdd1.cartesian(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cogroup\n",
    "# rdd1.cogroup(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f6a623343236782b3104312a38d731a14c125bf838f4c82db90ae7c3338775f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
